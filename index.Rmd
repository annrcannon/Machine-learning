---
title: "Project"
author: "Ann Cannon"
date: "3/22/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
library(caret)
library(rattle)

```

First we read in the training data, change the class of numeric variables from character to numeric, delete the first seven columns (because they won't help with the model) and eliminate any columns with missing values.
```{r, warning=FALSE, message=FALSE}
initialData <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")

initialData[,c(7:159)] <- sapply(initialData[,c(7:159)], as.numeric)
initialData <- initialData[,c(8:160)]

#Find the variables without missing values
vec <- 0
for(i in 1:153) {
      vec[i] <- sum(is.na(initialData[,i]))
}

noMissing <- initialData[,vec==0]

```

Next we divide the training data set into two parts: a training part and a validation part (called testing here).  We also force the \textit(classe) variable to be a factor variable.

```{r, warning=FALSE, message=FALSE}
inTrain <- createDataPartition(y=noMissing$classe, p=.75, list=FALSE)
training <- noMissing[inTrain,]
testing <- noMissing[-inTrain,]

training$classe <- factor(training$classe)
testing$classe <- factor(testing$classe)
```

Finally we begin the model building process by fitting a simple rpart model using all available variables.

```{r}
modFit <- train(classe~., data=training, method="rpart")
predicts <- predict(modFit, newdata=testing)

confusionMatrix(predicts, testing$classe)
fancyRpartPlot(modFit$finalModel)

```

This is an interesting start.  The model correctly predicts 49.8% of the validation sample.  But it doesn't classify anything as being in group D.

So we try a model using linear discriminant analysis, using the same variables.

```{r, warning=FALSE, message=FALSE}
modFitld <- train(classe~., data=training, method="lda")
predlda <- predict(modFitld, newdata=testing)

confusionMatrix(predlda, testing$classe)

```

This is better with an accuracy of 71%.  But we can do better yet.

```{r, warning=FALSE, message=FALSE}
modFitgbm <- train(classe~., data=training, method="gbm", verbose=FALSE)
predgbm <- predict(modFitgbm, newdata=testing)

confusionMatrix(predgbm, testing$classe)

```

Now we have 96% accuracy which means we have about 4% out of sample error.  WE will stay with this model.